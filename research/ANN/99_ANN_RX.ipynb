{"cells":[{"cell_type":"markdown","metadata":{"id":"zrBq0JTxkjG4"},"source":["This is the code for artificial neural networks training by Raymond Xu.\n","\n"," In this edition, the longitude and latitude of the original wildfire dataset is used as inputs.\n","\n"," Here the temporal sequence is not considered in the data splitting.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zMr8wP-kjG6"},"outputs":[],"source":["''' This is Research Project titled ML Algorithms for Alberta Forest Occurence Prediction.\n","    This is the 8th Engineering Research Project, and is hereby delcared as\n","\n","                            Project Christopher\n","\n","    Version 1.0 - Artificial Neural Network\n","    Data Source: European Space Agency - ERA5\n","                 Government of Alberta - Historical Wildfire registry and Fire Weather Indices\n","                 Natural Resources Canada - Vegetation Classification of Canada\n","\n","    AI Diederik - Hongik Ingan, For the Benefits of All Mankind\n","'''\n","\n","import math\n","import numpy as np\n","import pandas as pd\n","from math import floor\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import LSTM,Dense\n","from sklearn import set_config\n","from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder,LabelEncoder\n","set_config(transform_output = \"pandas\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc3Gl2zvkjG6"},"outputs":[],"source":["\n","# Load wildfire and non-wildfire datasets\n","main_df = pd.read_csv(\"G:/Shared drives/MECE 788 - Forest Fire Prediction/04_Preprocessing/Cleanup_and_point_selection/downsampled_df.csv\",index_col=0)\n","#main_df = pd.read_csv('downsampled_df.csv')\n","\n","# Remove the first unnamed column\n","wf_df=main_df[main_df['fire']==1]\n","nwf_df=main_df[main_df['fire']==0]\n"]},{"cell_type":"markdown","metadata":{"id":"lzcE23ZDkjG7"},"source":["Proceed splitting the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8tCns0WkjG7"},"outputs":[],"source":["\n","# Load wildfire and non-wildfire datasets\n","#main_df = pd.read_csv(\"G:/Shared drives/MECE 788 - Forest Fire Prediction/04_Preprocessing/Cleanup_and_point_selection/downsampled_df.csv\",index_col=0)\n","\n","# Remove the first unnamed column\n","wf_df=main_df[main_df['fire']==1]\n","nwf_df=main_df[main_df['fire']==0]\n","\n","# Define Split ratio, aka percentage of the combined data goes to training\n","split=[0.6,0.2,0.2]\n","wf_sort = wf_df.sort_values(by='date')\n","nwf_sort = nwf_df.sort_values(by='date')\n","\n","\n","for i in [0,1,2]:\n","    wf_memory=0\n","    nfw_memory=0\n","    wf_selected=[]\n","    nwf_selected=[]\n","    wf_size = int(split[i] * len(wf_df))\n","    nwf_size = int(split[i] * len(nwf_df))\n","    if i==0:\n","        wf_selected=wf_sort[:wf_size]\n","        nwf_selected=nwf_sort[:nwf_size]\n","        wf_memory=wf_size\n","        nwf_memory=nwf_size\n","        train_data = pd.concat([wf_selected, nwf_selected])\n","    if i==1:\n","        wf_selected=wf_sort[wf_memory:wf_memory+wf_size]\n","        nwf_selected=nwf_sort[nwf_memory:nwf_memory+nwf_size]\n","        wf_memory=wf_size\n","        nwf_memory=nwf_size\n","        test_data = pd.concat([wf_selected, nwf_selected])\n","    if i==2:\n","        wf_selected=wf_sort[wf_memory:]\n","        nwf_selected=nwf_sort[nwf_memory:]\n","        val_data = pd.concat([wf_selected, nwf_selected])\n","\n","X_train = train_data.drop(columns={'fire','date'})\n","X_test = test_data.drop(columns={'fire','date'})\n","X_val = val_data.drop(columns={'fire','date'})\n","y_train = train_data['fire']\n","y_test = test_data['fire']\n","y_val = val_data['fire']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1713131575196,"user":{"displayName":"Mohammed Shah","userId":"03051368408687709403"},"user_tz":360},"id":"c2W8j1ookjG7","outputId":"d1c06d43-a394-4cea-83ed-d8c42f3ee98d"},"outputs":[{"data":{"text/plain":["Index(['latitude', 'longitude', 'high_vegetation_cover',\n","       'leaf_area_index_high_vegetation', 'leaf_area_index_low_vegetation',\n","       'low_vegetation_cover', 'slope_of_sub_gridscale_orography',\n","       'type_of_high_vegetation', 'type_of_low_vegetation',\n","       '24hr_accumulated_precipitation', '24hr_max_temperature',\n","       'global_noon_LST_2m_temperature', 'global_noon_LST_relative_humidity',\n","       'global_noon_LST_wind_speed', 'BUI', 'DC', 'DMC', 'FFMC', 'FWI',\n","       'fire_count_past_3Days', 'fire_count_past_7Days',\n","       'fire_count_past_10Days', 'fire_count_past_30Days',\n","       '24hr_max_temperature_1dayLag', '24hr_max_temperature_2dayLag',\n","       'global_noon_LST_2m_temperature_1dayLag',\n","       'global_noon_LST_2m_temperature_2dayLag'],\n","      dtype='object')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["X_train.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zAixirj3kjG8"},"outputs":[],"source":["# Manual separate numerical and categorical columns\n","pass_features = [ 'leaf_area_index_high_vegetation', 'leaf_area_index_low_vegetation', 'slope_of_sub_gridscale_orography']\n","categorical_features = ['type_of_high_vegetation', 'type_of_low_vegetation']\n","numeric_features = train_data.drop(columns=pass_features).drop(columns=categorical_features).keys().drop(['fire','date'])\n","#numeric_features = numeric_features.insert(-1,'distance_to_road') Resevered for future development\n","feature_names =['numeric__fire_count_past_30Days','numeric__DMC','numeric__global_noon_LST_2m_temperature','numeric__BUI',\n","                'numeric__FWI','numeric__latitude','numeric__FFMC','numeric__global_noon_LST_relative_humidity','numeric__24hr_max_temperature',\n","                'numeric__global_noon_LST_2m_temperature_1dayLag','pass__leaf_area_index_high_vegetation','numeric__global_noon_LST_2m_temperature_2dayLag',\n","                'numeric__high_vegetation_cover','numeric__24hr_max_temperature_1dayLag','numeric__low_vegetation_cover','pass__leaf_area_index_low_vegetation',\n","                'numeric__24hr_accumulated_precipitation']\n","\n","# Define numeric and categorical transformer below\n","scale=ColumnTransformer([('scale_transformer',StandardScaler(),numeric_features)],verbose_feature_names_out=False).set_output(transform='pandas')\n","\n","cate=ColumnTransformer([('categorical_transformer',OneHotEncoder(sparse_output=False),categorical_features)],verbose_feature_names_out=False).set_output(transform='pandas')\n","\n","pss=ColumnTransformer([('Pass_transformer','passthrough',pass_features)],verbose_feature_names_out=False).set_output(transform='pandas')\n","Drop_transformer=ColumnTransformer([('Drop_transformer','passthrough',feature_names)],verbose_feature_names_out=False).set_output(transform='pandas')\n","\n","Data_pipeline = Pipeline(steps=[\n","    ('Feature Union',FeatureUnion([('numeric', scale),('categorical',cate),('pass',pss)])),\n","    ('Drop Columns',Drop_transformer)]\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5PZT6xlkjG8"},"outputs":[],"source":["X_train_processed = Data_pipeline.fit_transform(X_train)\n","X_test_processed = Data_pipeline.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPu8C1jUkjG8"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Dense\n","\n","from keras_tuner import Hyperband\n","import keras_tuner as kt\n","import tensorflow.keras.backend as K\n","from keras.metrics import AUC\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vTYrZTJ3puA"},"outputs":[],"source":["\n","def ANN_classifier(hp):\n","    num_input_val=X_train_processed.shape[1]\n","    model = Sequential()\n","    model.Input_shape=(X_train_processed.shape[1],)\n","    layers.Dense(units=hp.Int(f'units_0',min_value=num_input_val*2,max_value=num_input_val*15,step=num_input_val),\n","                 activation=hp.Choice('activation_0',['relu','tanh','sigmoid','swish','linear']),\n","                 input_shape=(X.shape[1],))\n","\n","        # Tune the number of layers.\n","    for i in range(hp.Int(\"num_layers\", 1,2,3)):\n","        model.add(Dense(units=hp.Int(f'units_{i}',min_value=num_input_val*2,max_value=num_input_val*15,step=num_input_val),\n","            activation=hp.Choice('activation_{i}',['relu','tanh','sigmoid','swish','linear'])))\n","        if i>=2 and hp.Boolean(\"dropout\"):# Tune whether to use dropout.\n","            model.add(layers.Dropout(rate=hp.Int(f'dropout{i}',min_value=0.2,max_value=0.5,step=0.1)))\n","\n","    # Define output layer\n","    model.add(Dense(1,activation='sigmoid'))\n","\n","    # Define the optimizer learning rate as a hyperparameter.\n","    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='binary_crossentropy', metrics=[keras.metrics.BinaryAccuracy(),keras.metrics.AUC()])\n","    return model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xL06skuekjG9"},"source":["Setup Hyperband Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1713131341072,"user":{"displayName":"Mohammed Shah","userId":"03051368408687709403"},"user_tz":360},"id":"brbNrtikkjG9","outputId":"c257f4bc-253e-4af5-b89a-83335adf41b2"},"outputs":[{"ename":"NameError","evalue":"name 'ANN_classifier' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mHyperband(\u001b[43mANN_classifier\u001b[49m,\n\u001b[0;32m      2\u001b[0m                      objective\u001b[38;5;241m=\u001b[39mkt\u001b[38;5;241m.\u001b[39mObjective(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m\"\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      3\u001b[0m                      max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      4\u001b[0m                      factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      5\u001b[0m                      executions_per_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      6\u001b[0m                      overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m                      directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANN_Hyperband_tuning\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                      project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANN_Raymond_Xu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(X_train_processed, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test_processed, y_test))\n","\u001b[1;31mNameError\u001b[0m: name 'ANN_classifier' is not defined"]}],"source":["tuner = kt.Hyperband(ANN_classifier,\n","                     objective=kt.Objective(\"val_auc\", direction=\"max\"),\n","                     max_epochs=10,\n","                     factor=3,\n","                     executions_per_trial=2,\n","                     overwrite=False,\n","                     directory='ANN_Hyperband_tuning',\n","                     project_name='ANN_Raymond_Xu')\n","\n","tuner.search(X_train_processed, y_train, epochs=10, validation_data=(X_test_processed, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_wld5jJkjG9"},"outputs":[],"source":["best_classifiers = tuner.get_best_models(num_models=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzhtMtJ3kjG9"},"outputs":[],"source":["best_classifier=best_classifiers[0]\n","best_classifier.fit(x=X_train_processed, y=y_train, batch_size=64, epochs=5, verbose=1, validation_split=0.2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ee579x8XkjG9"},"outputs":[],"source":["best_classifier.save('ANN_selected parameters_RX.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NjgZbuK5kjG9"},"outputs":[],"source":["# load fitted model\n","# best_classifier = tfk__load_model('ANN_selected parameters_RX.h5')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
