{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1k_cpFIiq0H2gxkMzJaXj-1YqwNTa_fjP","authorship_tag":"ABX9TyP9Xl0nBfP7lESwfKLMRpNg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","df = pd.read_csv(\"downsampled_df.csv\")\n","\n","X = df.drop(['fire', 'date'], axis=1)\n","y = df['fire']"],"metadata":{"id":"560HL_IzkHv_","executionInfo":{"status":"ok","timestamp":1712528122459,"user_tz":360,"elapsed":1884,"user":{"displayName":"Mohammed Shah","userId":"01087127448814409006"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# One hot encoding for categorical variables\n","vegetation_dummies = pd.get_dummies(X[['type_of_high_vegetation', 'type_of_low_vegetation', 'high_vegetation_cover', 'leaf_area_index_high_vegetation', 'leaf_area_index_low_vegetation', 'low_vegetation_cover' ]])\n","X = pd.concat([X, vegetation_dummies], axis=1)\n","X.drop(['type_of_high_vegetation'], axis=1, inplace=True)\n","\n","# Scaling most of the variables\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Update X with the scaled values\n","X = pd.DataFrame(X_scaled, columns=X.columns)\n"],"metadata":{"id":"EECbIxN_mmrw","executionInfo":{"status":"ok","timestamp":1712536682584,"user_tz":360,"elapsed":178,"user":{"displayName":"Mohammed Shah","userId":"01087127448814409006"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from sklearn.metrics import accuracy_score\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(128, activation='relu', input_shape=(X.shape[1],)),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(32, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"],"metadata":{"id":"jDO5orvUoZiV","executionInfo":{"status":"ok","timestamp":1712536685985,"user_tz":360,"elapsed":183,"user":{"displayName":"Mohammed Shah","userId":"01087127448814409006"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["history = model.fit(X_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n","\n","y_pred = model.predict(X_test)\n","\n","y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","# Calculate accuracy\n","from sklearn.metrics import accuracy_score\n","accuracy = accuracy_score(y_test, y_pred_binary)\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","from sklearn.metrics import accuracy_score\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"id":"DqkTcTTHqfuH","executionInfo":{"status":"error","timestamp":1712536697837,"user_tz":360,"elapsed":8750,"user":{"displayName":"Mohammed Shah","userId":"01087127448814409006"}},"outputId":"ef37225e-f311-4c42-a211-8c25b460b9ca"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1401/1404 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.5407"]},{"output_type":"error","ename":"ValueError","evalue":"Data cardinality is ambiguous:\n  x sizes: 11231\n  y sizes: 0\nMake sure all arrays contain the same number of samples.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-1b4474c01890>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Since the output of predict is probabilities, we need to convert them to binary predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             )\n\u001b[1;32m   1959\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 11231\n  y sizes: 0\nMake sure all arrays contain the same number of samples."]}]}]}